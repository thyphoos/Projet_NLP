{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_NLP",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBzaw42Boavn"
      },
      "source": [
        "# Importation des librairies et datas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjn0pzXDoWHQ"
      },
      "source": [
        "from keras.datasets import imdb\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from keras.layers import LSTM, Activation, Dropout, Dense, Input, Masking, Embedding, Flatten\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.models import Model\r\n",
        "import string\r\n",
        "import re\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import keras\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn import linear_model\r\n",
        "from keras.models import Sequential\r\n",
        "import pickle\r\n",
        "\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "import requests\r\n",
        "import json\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zunp57IQGUPt",
        "outputId": "2d53aac8-d122-4dad-998e-13d2858991fc"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2VoQvjgGcyc"
      },
      "source": [
        "data = pd.read_csv('gdrive/MyDrive/Nlp_projet/data_nlp.csv',encoding='ISO-8859-1',header=None,names=['feeling','ids','date','query','user','text'])\r\n",
        "data.drop(['ids','date','query','user'], axis=1, inplace=True) #on garde les colonnes qui nous intéressent\r\n",
        "data_size = 10000\r\n",
        "\r\n",
        "#sélection de 20000 échantillons pour accélerer les tests\r\n",
        "negative_data=data['text'][:data_size]\r\n",
        "positive_data=data['text'][800000:800000+data_size]\r\n",
        "\r\n",
        "negative_feelings=data['feeling'][:data_size]\r\n",
        "positive_feelings=data['feeling'][800000:800000+data_size]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IlGJyHFiAcx"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "WSSkfDZxHhYU",
        "outputId": "d248407e-7bb8-4c86-e9ea-db56fa8c85c1"
      },
      "source": [
        "#Remplissage de la liste indices sentiments\r\n",
        "def analyze(data):\r\n",
        "  feelings_col=[]\r\n",
        "  for line in data:\r\n",
        "    feelings_col.append(line)\r\n",
        "\r\n",
        "  return feelings_col\r\n",
        "\r\n",
        "positive_feelings_data = analyze(positive_feelings)\r\n",
        "negative_feelings_data = analyze(negative_feelings)\r\n",
        "feelings_data = positive_feelings_data + negative_feelings_data\r\n",
        "\r\n",
        "\r\n",
        "#Régulation de la ponctuation \r\n",
        "def punct_remover(data):\r\n",
        "  removed_punct = string.punctuation\r\n",
        "  punct=[]\r\n",
        "  for i in range(len(removed_punct)):\r\n",
        "    punct.append(removed_punct[i])\r\n",
        "\r\n",
        "\r\n",
        "  no_punct_dataset=[]\r\n",
        "  element=\"\"\r\n",
        "  for elt in data:\r\n",
        "    for letter in elt:\r\n",
        "      if letter not in punct and not letter.isdigit():\r\n",
        "        element += letter\r\n",
        "\r\n",
        "    no_punct_dataset.append(element)\r\n",
        "    element =\"\"\r\n",
        "  \r\n",
        "  return no_punct_dataset\r\n",
        "\r\n",
        "Analyzed_positive_data=punct_remover(positive_data)\r\n",
        "Analyzed_negative_data=punct_remover(negative_data)\r\n",
        "Analyzed_data=Analyzed_positive_data + Analyzed_negative_data\r\n",
        "\r\n",
        "#Création d'un dataframe texte+indice sentiment\r\n",
        "frame = pd.DataFrame(Analyzed_data, columns = [\"text\"])\r\n",
        "frame[\"feelings\"] = feelings_data\r\n",
        "frame"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>feelings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I LOVE HealthUandPets u guys r the best</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>im meeting up with one of my besties tonight C...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DaRealSunisaKim Thanks for the Twitter add Sun...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Being sick can be really cheap when it hurts t...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LovesBrooklyn he has that effect on everyone</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>Aww thats sad</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>stupid dvds stuffing up the good bits in jaws</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>DandySephy No Only close friends and family Im...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>CRAP After looking when I last tweeted WHY AM ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>Its Another Rainboot day</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  feelings\n",
              "0               I LOVE HealthUandPets u guys r the best          4\n",
              "1      im meeting up with one of my besties tonight C...         4\n",
              "2      DaRealSunisaKim Thanks for the Twitter add Sun...         4\n",
              "3      Being sick can be really cheap when it hurts t...         4\n",
              "4          LovesBrooklyn he has that effect on everyone          4\n",
              "...                                                  ...       ...\n",
              "19995                                     Aww thats sad          0\n",
              "19996     stupid dvds stuffing up the good bits in jaws          0\n",
              "19997  DandySephy No Only close friends and family Im...         0\n",
              "19998  CRAP After looking when I last tweeted WHY AM ...         0\n",
              "19999                          Its Another Rainboot day          0\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "yZbwCYdpgWra",
        "outputId": "a1807ccb-e8c9-48d0-8f1f-91c08335acdc"
      },
      "source": [
        "#Mise en minuscule des textes\r\n",
        "def data_lower(data):\r\n",
        "  for i in range(len(data[\"text\"])):\r\n",
        "    data[\"text\"][i]=data[\"text\"][i].lower()\r\n",
        "  \r\n",
        "  return data\r\n",
        "\r\n",
        "lower_frame = data_lower(frame)\r\n",
        "lower_frame"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>feelings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i love healthuandpets u guys r the best</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>im meeting up with one of my besties tonight c...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>darealsunisakim thanks for the twitter add sun...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>being sick can be really cheap when it hurts t...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lovesbrooklyn he has that effect on everyone</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>aww thats sad</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>stupid dvds stuffing up the good bits in jaws</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>dandysephy no only close friends and family im...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>crap after looking when i last tweeted why am ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>its another rainboot day</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  feelings\n",
              "0               i love healthuandpets u guys r the best          4\n",
              "1      im meeting up with one of my besties tonight c...         4\n",
              "2      darealsunisakim thanks for the twitter add sun...         4\n",
              "3      being sick can be really cheap when it hurts t...         4\n",
              "4          lovesbrooklyn he has that effect on everyone          4\n",
              "...                                                  ...       ...\n",
              "19995                                     aww thats sad          0\n",
              "19996     stupid dvds stuffing up the good bits in jaws          0\n",
              "19997  dandysephy no only close friends and family im...         0\n",
              "19998  crap after looking when i last tweeted why am ...         0\n",
              "19999                          its another rainboot day          0\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-eccaOsiAJQ",
        "outputId": "9e46c893-1782-42a2-9737-3f6a87cf015c"
      },
      "source": [
        "#Le texte est transformé en séquence d'entiers puis un padding est appliqué\r\n",
        "tokenizer = Tokenizer(num_words=data_size, split=\" \")\r\n",
        "tokenizer.fit_on_texts(lower_frame[\"text\"].values)\r\n",
        "Vectorized_text = tokenizer.texts_to_sequences(lower_frame[\"text\"].values)\r\n",
        "\r\n",
        "Vectorized_text = pad_sequences(X)\r\n",
        "print(Vectorized_text[:33])\r\n",
        "print(Vectorized_text.shape)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  388    3  179]\n",
            " [   0    0    0 ...  142  294  324]\n",
            " [   0    0    0 ...  131    4 2092]\n",
            " ...\n",
            " [   0    0    0 ...   25    3  422]\n",
            " [   0    0    0 ...  712   18  109]\n",
            " [   0    0    0 ... 9500 9501 9502]]\n",
            "(20000, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxuxnqSPnjCE",
        "outputId": "5933271f-d48c-4324-da72-58458a8d2306"
      },
      "source": [
        "#Retourne des tuples positifs/négatifs ([1 0] négatif et [0 1] positif)\r\n",
        "tuple_feelings = pd.get_dummies(frame[\"feelings\"]).values\r\n",
        "print(tuple_feelings)\r\n",
        "print(tuple_feelings.shape)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " ...\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]]\n",
            "(20000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E24d8gqNooen"
      },
      "source": [
        "# Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK6CCPvEnnJP",
        "outputId": "54a6acbb-953c-45c2-e795-dde5a5c4820f"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "#Transforme les entiers positifs (index) en vecteurs de tailles fixes\r\n",
        "#Une régulation sur le réseau est appliquée\r\n",
        "model.add(Embedding(data_size, 256, input_length=X.shape[1]))\r\n",
        "model.add(Dropout(0.3))\r\n",
        "\r\n",
        "#Utilisation d'un réseau de longue mémoire à courte terme pour allonger la mémoire\r\n",
        "model.add(LSTM(256,return_sequences=False,dropout=0.3,recurrent_dropout=0.2))\r\n",
        "model.add(Dense(2, activation=\"sigmoid\"))\r\n",
        "\r\n",
        "model.compile(\r\n",
        "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "#On sépare la data entre l'entrainement et la validation du modèle \r\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,train_size=0.7,random_state=0)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 32, 256)           5120000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 5,645,826\n",
            "Trainable params: 5,645,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "petNlBTNo5Qe"
      },
      "source": [
        "# Entrainement et sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbQ_fMvHnn2B",
        "outputId": "fb5b76da-fd42-471b-8055-cc948d0d969e"
      },
      "source": [
        "model.fit(x_train,  y_train, \r\n",
        "                   epochs=8, batch_size=100,verbose =2)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "140/140 - 94s - loss: 0.5911 - accuracy: 0.6738\n",
            "Epoch 2/8\n",
            "140/140 - 90s - loss: 0.4422 - accuracy: 0.8012\n",
            "Epoch 3/8\n",
            "140/140 - 91s - loss: 0.3600 - accuracy: 0.8417\n",
            "Epoch 4/8\n",
            "140/140 - 91s - loss: 0.2860 - accuracy: 0.8817\n",
            "Epoch 5/8\n",
            "140/140 - 91s - loss: 0.2294 - accuracy: 0.9073\n",
            "Epoch 6/8\n",
            "140/140 - 92s - loss: 0.1847 - accuracy: 0.9250\n",
            "Epoch 7/8\n",
            "140/140 - 102s - loss: 0.1510 - accuracy: 0.9395\n",
            "Epoch 8/8\n",
            "140/140 - 91s - loss: 0.1284 - accuracy: 0.9509\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa9b2244a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmewcFointRf",
        "outputId": "71873fff-684d-47f8-84e6-727b6c58c780"
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "188/188 [==============================] - 9s 48ms/step - loss: 0.9639 - accuracy: 0.7195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9639269113540649, 0.7195000052452087]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTs3Cql4rvaW",
        "outputId": "73ba0ce7-2ee1-4572-9e1c-59ef321abace"
      },
      "source": [
        "model.save('/content/gdrive/MyDrive/Nlp_projet/Model1')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Nlp_projet/Model1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3krsDw7QpJhA"
      },
      "source": [
        "# Récupération d'informations de L' API Yelp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJz2PRkynv2A",
        "outputId": "f6c8c1d4-a3f7-4f94-ef5c-763d646429a5"
      },
      "source": [
        "api_key='q1HJoEmRMdypie_E6lYwiih0hNfuSatSTQXH6Bh9fnltm30JX4XO1TSCd6NARRVszrs4lMXb7N3i6LWB_UIBEinyG_ah-0expG6GUnzTdUss2gvNQRcJ7fJhK5g2YHYx'\r\n",
        "headers = {'Authorization': 'Bearer %s' % api_key}\r\n",
        "\r\n",
        "\r\n",
        "url = \"https://api.yelp.com/v3/businesses/FEVQpbOPOwAPNIgO7D3xxw/reviews\"\r\n",
        "req = requests.get(url, headers=headers)\r\n",
        "print('the status code is {}'.format(req.status_code))\r\n",
        "\r\n",
        "\r\n",
        "data=json.loads(req.text)\r\n",
        "data"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the status code is 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'possible_languages': ['fr', 'en', 'zh', 'pt', 'de', 'it', 'sv', 'ja', 'es'],\n",
              " 'reviews': [{'id': 'qOiuCTz_Gzgmose1GHaAlg',\n",
              "   'rating': 5,\n",
              "   'text': \"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\",\n",
              "   'time_created': '2020-12-19 07:52:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=qOiuCTz_Gzgmose1GHaAlg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': 'FRKnLBnlFiasr1Dc8oOIGQ',\n",
              "    'image_url': 'https://s3-media3.fl.yelpcdn.com/photo/1wm0yjWSw92j_ZsUFZBGzQ/o.jpg',\n",
              "    'name': 'Sarah G.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=FRKnLBnlFiasr1Dc8oOIGQ'}},\n",
              "  {'id': '7d56A_ObMPHHyywcfhnrUw',\n",
              "   'rating': 5,\n",
              "   'text': 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...',\n",
              "   'time_created': '2021-02-25 19:56:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=7d56A_ObMPHHyywcfhnrUw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': '77MIRZBZqbQKoonun8qWjQ',\n",
              "    'image_url': 'https://s3-media2.fl.yelpcdn.com/photo/ptL063kjqRUDQ6pny96iqg/o.jpg',\n",
              "    'name': 'Ipsita D.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=77MIRZBZqbQKoonun8qWjQ'}},\n",
              "  {'id': 'SYykUNBT2dJznMV5PpRnhg',\n",
              "   'rating': 5,\n",
              "   'text': 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...',\n",
              "   'time_created': '2020-12-12 15:19:49',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=SYykUNBT2dJznMV5PpRnhg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': 'yw2cJk_SfGZlcoZKEUevxw',\n",
              "    'image_url': 'https://s3-media1.fl.yelpcdn.com/photo/EH2WTffzgKs72GHqa4kn6A/o.jpg',\n",
              "    'name': 'Thomas V.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=yw2cJk_SfGZlcoZKEUevxw'}}],\n",
              " 'total': 5609}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqU4fWF9pmEJ"
      },
      "source": [
        "# Traitement des informations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adY49cjlny9U"
      },
      "source": [
        "#Application du preprocessing sur les informations importées\r\n",
        "def traitement(tweet):\r\n",
        "    \r\n",
        "  user_text=punct_remover(tweet)\r\n",
        "\r\n",
        "  frame_string=pd.DataFrame(user_text, columns = [\"text\"])\r\n",
        "  test_lower=data_lower(frame_string)\r\n",
        "\r\n",
        "  tokenizer = Tokenizer(num_words=data_size, split=\" \")\r\n",
        "  tokenizer.fit_on_texts(test_lower[\"text\"].values)\r\n",
        "\r\n",
        "  pp_string = tokenizer.texts_to_sequences(test_lower[\"text\"].values)\r\n",
        "  pp_string = pad_sequences(pp_string)\r\n",
        "\r\n",
        "  return pp_string"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxtg27oepzJM"
      },
      "source": [
        "# Utilisation du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwntsF7En1Zk",
        "outputId": "f16e6583-50ff-4a43-dd70-6d6e082a0f31"
      },
      "source": [
        "text1 =data[\"reviews\"][0][\"text\"]\r\n",
        "text2=\"I went to this cupcakes shop after the recommendations I got from the comments and I am utterly disappointed. Whether they made their comments themselves or people have a very bad sense of taste but I do not recommend any of you to go there.\"\r\n",
        "text3=\"Very good restaurant with a lovely staff.\"  \r\n",
        "\r\n",
        "test1 = traitement(text1)\r\n",
        "test2 = traitement(text2)\r\n",
        "test3 = traitement(text3)\r\n",
        "\r\n",
        "preds1 = model.predict(test1)\r\n",
        "preds2 = model.predict(test2)\r\n",
        "preds3 = model.predict(test3)\r\n",
        "prediction_list=[preds1,preds2,preds3]\r\n",
        "target_feeling=[\"positive\",\"negative\",\"positive\"]\r\n",
        "\r\n",
        "for i in range(len(prediction_list)):\r\n",
        "  if prediction_list[i][0][0] < (1/3):\r\n",
        "    print('predicted feeling : negative')\r\n",
        "    print(f\"target feeling : {target_feeling[i]}\")\r\n",
        "  elif  prediction_list[i][0][0] > (2/3): \r\n",
        "    print('predicted feeling : positive')\r\n",
        "    print(f\"target feeling : {target_feeling[i]}\")\r\n",
        "  else:\r\n",
        "    print('predicted feeling : neutral')\r\n",
        "    print(f\"target feeling : {target_feeling[i]}\")"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted feeling : positive\n",
            "target feeling : positive\n",
            "predicted feeling : positive\n",
            "target feeling : negative\n",
            "predicted feeling : neutral\n",
            "target feeling : positive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}